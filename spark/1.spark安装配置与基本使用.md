## 一、实验名称

spark的安装配置与基本使用

## 二、实验目的

掌握spark的安装配置

## 三、实验环境

centos7

hadoop3.3.4

spark3.3.2

## 四、实验内容及步骤

### 1. local模式

#### (1)  解压安装包

```shell
tar -zxvf spark-3.3.2-bin-hadoop3.tgz -C /opt/module
cd /opt/module 
mv spark-3.3.2-bin-hadoop3 spark
```

#### (2) 启动local模式

```shell
bin/spark-shell
```

![](pic\1local启动.png)

#### (3)测试wordcount

##### 1）样例数据

```
banzhang ni hao
xihuan hadoop banzhang
banzhang ni hao
xihuan hadoop banzhang
```

##### 2)  在hadoop101这台机器上新建测试数据

```shell
vim /opt/data/test_data/hello.txt
# 把1)中的数据写入hello.txt中
```

##### 3）在spark-shell中执行算子，统计字数

```shell
sc.textFile("/opt/data/test_data/hello.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect
```

![](pic\2测试workcount.png)

##### 4）退出本地shell模式

![](pic\3退出shell.png)

5）运行spark自带的demo（计算圆周率）

```shell
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[2] \
--deploy-mode cluster \
./examples/jars/spark-examples_2.12-3.3.2.jar 10

```

![](pic\4运行圆周率.png)

![5本地模式测试圆周率](pic\5本地模式测试圆周率.png)



### 2.Standalone模式

#### (1)复制spark，重命名为

```shell
cd /opt/module
mv spark spark-standalone
```

#### (2)配置spark-standalone

##### 1) 进入配置文件所在的文件夹

```shell
cd /opt/module/spark/conf
```

##### 2) 配置workers

```shell
#配置workers
mv workers.template workers
#在workers
# 注释掉localhost
hadoop101
hadoop102
hadoop103
```

##### 3）配置spark-env.sh

```shell
export JAVA_HOME=/opt/kit/jdk8
SPARK_MASTER_HOST=hadoop101
# 注意：7077端口，相当于hadoop3内部通信的9000端口，此处的端口需要确认自己的Hadoop配置
SPARK_MASTER_PORT=7077
```

4）分发spark-standalone目录给hadoop102和hadoop103这两台机器

```shell
scp -r spark-standalone/ hp@hadoop102"/opt/module/
scp -r spark-standalone/ hp@hadoop102"/opt/module/
```

5）启动spark-standalone集群

```shell
# 进入spark-standalone home目录下
sbin/start-all.sh
```

![](pic\6spark-standalone启动.png)

![](pic\7spark-standalone-web界面.png)

![](pic\8查看各个节点启动.png)

4）测试圆周率demo

```shell
bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://hadoop101:7070 \
--deploy-mode cluster \
./examples/jars/spark-examples_2.12-3.3.2.jar 10
```

![](pic\10圆周率结果.png)

### 3. yarn模式

#### (1). 修改配置文件

```shell
# 进入spark_home下面的配置文件目录
cd /opt/module/spark/conf
vim spark-env.sh
# 添加或修改一下内容
```

```shell
export JAVA_HOME=/opt/kit/jdk8

export HADOOP_HOME=/opt/module/hadoop
export HADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoop
export YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop
export SPARK_DIST_CLASSPATH=$(/opt/module/hadoop/bin/hadoop classpath)
export SPARK_HISTORY_OPTS="
-Dspark.history.ui.port=18080 
-Dspark.history.retainedApplications=30 
-Dspark.history.fs.logDirectory=hdfs://192.168.79.101:9000/logs/spark-logs/"
```

